{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieBgxkP6P5QC"
      },
      "source": [
        "# **Tokenization**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TxJlfSlccS9"
      },
      "source": [
        "### **What is Tokenization?**\n",
        "\n",
        "Tokenization is breaking a sentence, paragraph, or an text document into smaller units, such as words or characters. Each of these smaller units are called tokens. The tokens could be words, numbers or punctuation marks. \n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Nbe2bOd5FA"
      },
      "source": [
        "### **Need of Tokenization**\n",
        "\n",
        "Tokenization is the most basic step to proceed with NLP. After tokenization the meaning of the text can be interpreted by analyzing the words present in the text. If we are given a paragraph, we need to get all sentences. From all these sentences, we need words and then only we can understand the text completely.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTswAFqUh1fm"
      },
      "source": [
        "### **Different methods to perform Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRIniUjPkSf2"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **1. Tokenization using Regular Expression**\n",
        "\n",
        "<hr/>\n",
        "\n",
        "A RegEx, or Regular Expression, is a sequence of characters that forms a search pattern. RegEx can be used to check if a string contains the specified search pattern. Python has a built-in package called re, which can be used to work with Regular Expressions.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Function      | Description |\n",
        "| ----------- | ----------- |\n",
        "| findall      | Returns a list containing all matches     |\n",
        "| search   | Returns a Match object if there is a match anywhere in the string        |\n",
        "| split      | Returns a list where the string has been split at each match |\n",
        "| sub         | Replaces one or many matches with a string\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Function     | Description |\n",
        "| ----------- | ----------- |\n",
        "| \\w     | Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)      |\n",
        "|  \\W     |  Returns a match where the string DOES NOT contain any word characters |\n",
        "| \\d  | Returns a match where the string contains digits (numbers from 0-9)        |\n",
        "|   \\D      |   Returns a match where the string DOES NOT contain digits  |\n",
        "| \\s       |  Returns a match where the string contains a white space character |\n",
        "|  \\S      |   Returns a match where the string DOES NOT contain a white space character | \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMv0a1W9wlnz"
      },
      "source": [
        "#### **Word Tokenization**\n",
        "\n",
        "To split the sentences into words or tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6BPYbH1vh5J",
        "outputId": "a481b81c-fb25-4992-f028-0fa97c199320"
      },
      "source": [
        "#import the required library\n",
        "import re\n",
        "\n",
        "#Text to be tokenized\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "#Word Tokenization\n",
        "tokens = re.findall(\"[\\w']+\", text)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Robofied',\n",
              " 'is',\n",
              " 'a',\n",
              " 'comprehensive',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'platform',\n",
              " 'based',\n",
              " 'in',\n",
              " 'Gurugram',\n",
              " 'Haryana',\n",
              " 'working',\n",
              " 'towards',\n",
              " 'democratizing',\n",
              " 'safe',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'towards',\n",
              " 'a',\n",
              " 'common',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'Singularity',\n",
              " 'At',\n",
              " 'Robofied',\n",
              " 'we',\n",
              " 'are',\n",
              " 'doing',\n",
              " 'research',\n",
              " 'in',\n",
              " 'speech',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'We',\n",
              " 'develop',\n",
              " 'open',\n",
              " 'source',\n",
              " 'solutions',\n",
              " 'for',\n",
              " 'developers',\n",
              " 'which',\n",
              " 'empowers',\n",
              " 'them',\n",
              " 'so',\n",
              " 'that',\n",
              " 'they',\n",
              " 'can',\n",
              " 'make',\n",
              " 'better',\n",
              " 'products',\n",
              " 'for',\n",
              " 'the',\n",
              " 'world',\n",
              " 'We',\n",
              " 'educate',\n",
              " 'people',\n",
              " 'about',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'its',\n",
              " 'scope',\n",
              " 'and',\n",
              " 'impact',\n",
              " 'via',\n",
              " 'resources',\n",
              " 'and',\n",
              " 'tutorials']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tMb9vQWw7fJ"
      },
      "source": [
        "#### **Sentence Tokenization**\n",
        "\n",
        "To split a document or paragraph into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujrtXr3MxBZY",
        "outputId": "6ba6e1b3-2109-4217-89bf-5c9d88359c1a"
      },
      "source": [
        "#import the required library\n",
        "import re\n",
        "\n",
        "#Text to be tokenized\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "#Sentence Tokenization\n",
        "sentences = re.split(r'[.!?]',text)\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity',\n",
              " ' At Robofied, we are doing research in speech, natural language, and machine learning',\n",
              " ' We develop open-source solutions for developers which empowers them so that they can make better products for the world',\n",
              " ' We educate people about Artificial Intelligence, its scope and impact via resources and tutorials',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_arLswaL1Pgb"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **2. Tokenization using NLTK**\n",
        "\n",
        "<hr/>\n",
        "\n",
        "NLTK, short for Natural Language ToolKit, is a library written in Python for symbolic and statistical Natural Language Processing. \n",
        "NLTK contains a module called tokenize() which further classifies into two sub-categories:\n",
        "\n",
        "    1. Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words\n",
        "    2. Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7RqFZOGlACE"
      },
      "source": [
        "### **Word Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3gKi5YF4x4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254be191-e234-499e-d335-7426443f8780"
      },
      "source": [
        "#importing libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "#text to be tokenized\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "#Word Tokenization\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['Robofied', 'is', 'a', 'comprehensive', 'Artificial', 'Intelligence', 'platform', 'based', 'in', 'Gurugram', ',', 'Haryana', 'working', 'towards', 'democratizing', 'safe', 'artificial', 'intelligence', 'towards', 'a', 'common', 'goal', 'of', 'Singularity', '.', 'At', 'Robofied', ',', 'we', 'are', 'doing', 'research', 'in', 'speech', ',', 'natural', 'language', ',', 'and', 'machine', 'learning', '.', 'We', 'develop', 'open-source', 'solutions', 'for', 'developers', 'which', 'empowers', 'them', 'so', 'that', 'they', 'can', 'make', 'better', 'products', 'for', 'the', 'world', '.', 'We', 'educate', 'people', 'about', 'Artificial', 'Intelligence', ',', 'its', 'scope', 'and', 'impact', 'via', 'resources', 'and', 'tutorials', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC8xvQ9Yliv9"
      },
      "source": [
        "### **Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXf4_Jx0lmJ8",
        "outputId": "e33d2d56-d968-4a05-b1fa-180c0f752d97"
      },
      "source": [
        "#importing libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "#text to be tokenize\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "#Sentence Tokenization\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity.', 'At Robofied, we are doing research in speech, natural language, and machine learning.', 'We develop open-source solutions for developers which empowers them so that they can make better products for the world.', 'We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK3UvvQ1mzO7"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **3. Tokenization using SpaCy**\n",
        "\n",
        "<hr/>\n",
        "\n",
        "SpaCy is a free, open-source library for advanced Natural Language Processing in Python. spaCy comes with pretrained NLP models that can perform most common NLP tasks, such as tokenization, parts of speech (POS) tagging, named entity recognition (NER), lemmatization, transforming to word vectors etc.\n",
        " It supports over 49+ languages and is very fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iTtQblfvWuE"
      },
      "source": [
        "### **Word Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG6tX8AsvVqI",
        "outputId": "5ee5d4a9-e58f-416e-ba06-0159a4600b6f"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "\n",
        "my_doc = nlp(text)\n",
        "for token in my_doc:\n",
        "  print(token.text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Robofied\n",
            "is\n",
            "a\n",
            "comprehensive\n",
            "Artificial\n",
            "Intelligence\n",
            "platform\n",
            "based\n",
            "in\n",
            "Gurugram\n",
            ",\n",
            "Haryana\n",
            "working\n",
            "towards\n",
            "democratizing\n",
            "safe\n",
            "artificial\n",
            "intelligence\n",
            "towards\n",
            "a\n",
            "common\n",
            "goal\n",
            "of\n",
            "Singularity\n",
            ".\n",
            "At\n",
            "Robofied\n",
            ",\n",
            "we\n",
            "are\n",
            "doing\n",
            "research\n",
            "in\n",
            "speech\n",
            ",\n",
            "natural\n",
            "language\n",
            ",\n",
            "and\n",
            "machine\n",
            "learning\n",
            ".\n",
            "We\n",
            "develop\n",
            "open\n",
            "-\n",
            "source\n",
            "solutions\n",
            "for\n",
            "developers\n",
            "which\n",
            "empowers\n",
            "them\n",
            "so\n",
            "that\n",
            "they\n",
            "can\n",
            "make\n",
            "better\n",
            "products\n",
            "for\n",
            "the\n",
            "world\n",
            ".\n",
            "We\n",
            "educate\n",
            "people\n",
            "about\n",
            "Artificial\n",
            "Intelligence\n",
            ",\n",
            "its\n",
            "scope\n",
            "and\n",
            "impact\n",
            "via\n",
            "resources\n",
            "and\n",
            "tutorials\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBdCg_CswwQL"
      },
      "source": [
        "### **Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56VFZVzvw0vm",
        "outputId": "0eb91b6f-627e-450d-968a-87cfb67e2af5"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer\n",
        "nlp = English()\n",
        "\n",
        "# Create the pipeline 'sentencizer' component\n",
        "snt = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(snt)\n",
        "\n",
        "text = text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "  print(sent.text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity.\n",
            "At Robofied, we are doing research in speech, natural language, and machine learning.\n",
            "We develop open-source solutions for developers which empowers them so that they can make better products for the world.\n",
            "We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}