{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BstiSQbq5nmP"
      },
      "source": [
        "## **STOPWORDS**\r\n",
        "\r\n",
        "---\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiYc7D8T5uN3"
      },
      "source": [
        "### **What are stopwords?**\r\n",
        "\r\n",
        "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\r\n",
        "\r\n",
        "---\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5hVRAaH66hS"
      },
      "source": [
        "### **Need of stopwords**\r\n",
        "\r\n",
        "Tasks like text classification, where the text is to be classified into different categories, stopwords are removed from the given text so that more focus can be given to those words which define the meaning of the text.\r\n",
        "\r\n",
        "By removing stopwords,dataset size decreases and the time to train the model also decreases.\r\n",
        "Removing stopwords can help improve the performance as there are fewer and only meaningful tokens left. It could increase classification accuracy.\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3f7bE8O7uWt"
      },
      "source": [
        "### **Methods to remove stopwords**\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0cdFBOr8YEx"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **1. Stopwords removal using NLTK**\r\n",
        "\r\n",
        "<hr/>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrroCoox9HzE",
        "outputId": "ea1e8aaf-6861-4919-837e-c1c7f6708376"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize \r\n",
        "set(stopwords.words('english'))\r\n",
        "\r\n",
        "\r\n",
        "# sample sentence\r\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\r\n",
        "\r\n",
        "# set of stop words\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "\r\n",
        "# tokens of words  \r\n",
        "word_tokens = word_tokenize(text) \r\n",
        "    \r\n",
        "new_sentence = [] \r\n",
        "  \r\n",
        "for w in word_tokens: \r\n",
        "    if w not in stop_words: \r\n",
        "        new_sentence.append(w) \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(\"\\n\\nOriginal Sentence \\n\\n\")\r\n",
        "print(\" \".join(word_tokens)) \r\n",
        "\r\n",
        "print(\"\\n\\nNew Sentence \\n\\n\")\r\n",
        "print(\" \".join(new_sentence)) \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "\n",
            "Original Sentence \n",
            "\n",
            "\n",
            "Robofied is a comprehensive Artificial Intelligence platform based in Gurugram , Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity . At Robofied , we are doing research in speech , natural language , and machine learning . We develop open-source solutions for developers which empowers them so that they can make better products for the world . We educate people about Artificial Intelligence , its scope and impact via resources and tutorials .\n",
            "\n",
            "\n",
            "New Sentence \n",
            "\n",
            "\n",
            "Robofied comprehensive Artificial Intelligence platform based Gurugram , Haryana working towards democratizing safe artificial intelligence towards common goal Singularity . At Robofied , research speech , natural language , machine learning . We develop open-source solutions developers empowers make better products world . We educate people Artificial Intelligence , scope impact via resources tutorials .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4PIVMS5906J"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **2. Stopword Removal using Gensim**\r\n",
        "\r\n",
        "<hr/>\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy3ERqYE-DLD",
        "outputId": "60f68042-eb30-4cf3-f07b-30b805a77ffb"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\r\n",
        "\r\n",
        "# pass the sentence in the remove_stopwords function\r\n",
        "result = remove_stopwords(\"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\")\r\n",
        "\r\n",
        "print('\\n\\n New Sentence \\n\\n')\r\n",
        "print(result)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " New Sentence \n",
            "\n",
            "\n",
            "Robofied comprehensive Artificial Intelligence platform based Gurugram,Haryana working democratizing safe artificial intelligence common goal Singularity. At Robofied, research speech, natural language, machine learning. We develop open-source solutions developers empowers better products world. We educate people Artificial Intelligence, scope impact resources tutorials.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0B2UQ3z-Wl-"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **Stopword Removal using spaCy**\r\n",
        "\r\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaE0iWjF-fWz",
        "outputId": "88746d27-b35d-4291-ed22-5736bb366c88"
      },
      "source": [
        "from spacy.lang.en import English\r\n",
        "\r\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\r\n",
        "nlp = English()\r\n",
        "\r\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "doc = nlp(text)\r\n",
        "\r\n",
        "# Create list of word tokens\r\n",
        "token_list = []\r\n",
        "for token in doc:\r\n",
        "    token_list.append(token.text)\r\n",
        "\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
        "\r\n",
        "# Create list of word tokens after removing stopwords\r\n",
        "New_sentence =[] \r\n",
        "\r\n",
        "for word in token_list:\r\n",
        "    lexeme = nlp.vocab[word]\r\n",
        "    if lexeme.is_stop == False:\r\n",
        "        New_sentence.append(word) \r\n",
        "print(token_list)\r\n",
        "print(New_sentence)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Robofied', 'is', 'a', 'comprehensive', 'Artificial', 'Intelligence', 'platform', 'based', 'in', 'Gurugram', ',', 'Haryana', 'working', 'towards', 'democratizing', 'safe', 'artificial', 'intelligence', 'towards', 'a', 'common', 'goal', 'of', 'Singularity', '.', 'At', 'Robofied', ',', 'we', 'are', 'doing', 'research', 'in', 'speech', ',', 'natural', 'language', ',', 'and', 'machine', 'learning', '.', 'We', 'develop', 'open', '-', 'source', 'solutions', 'for', 'developers', 'which', 'empowers', 'them', 'so', 'that', 'they', 'can', 'make', 'better', 'products', 'for', 'the', 'world', '.', 'We', 'educate', 'people', 'about', 'Artificial', 'Intelligence', ',', 'its', 'scope', 'and', 'impact', 'via', 'resources', 'and', 'tutorials', '.']\n",
            "['Robofied', 'comprehensive', 'Artificial', 'Intelligence', 'platform', 'based', 'Gurugram', ',', 'Haryana', 'working', 'democratizing', 'safe', 'artificial', 'intelligence', 'common', 'goal', 'Singularity', '.', 'Robofied', ',', 'research', 'speech', ',', 'natural', 'language', ',', 'machine', 'learning', '.', 'develop', 'open', '-', 'source', 'solutions', 'developers', 'empowers', 'better', 'products', 'world', '.', 'educate', 'people', 'Artificial', 'Intelligence', ',', 'scope', 'impact', 'resources', 'tutorials', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}