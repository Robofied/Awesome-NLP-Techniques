{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JrR6hLElh5I"
      },
      "source": [
        "## **BAG OF WORDS**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKHMMrIUlqt2"
      },
      "source": [
        "## **What is Bag of Words?**\n",
        "\n",
        "Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. \n",
        "It is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. It keep track of word counts and disregard the grammatical details and the word order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8idKfH7oYAP"
      },
      "source": [
        "### **Why Bag of words is used?**\n",
        "\n",
        "With bag-of-Words we can convert variable-length texts into a fixed-length vector. Machine learning models work with numerical data. By using the bag of words technique, we can convert a text into its equivalent vector of numbers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEJXuG9_JrnA"
      },
      "source": [
        "### **How to apply bag of words?**\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGafphwln0iG"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **1. Tokenization**\r\n",
        "\r\n",
        "<hr/>\r\n",
        "\r\n",
        "Converting the sentence into tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBFiS_DMrmiv",
        "outputId": "10379455-226d-4c0f-e567-fd875c74871e"
      },
      "source": [
        "import nltk \r\n",
        "nltk.download('punkt')\r\n",
        "import re \r\n",
        "import numpy as np \r\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\r\n",
        "dataset = nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WlQgNgTuD7p"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **2. Preprocessing the Data**\r\n",
        "\r\n",
        "<hr/>\r\n",
        "\r\n",
        "1. Convert text to lower case.\r\n",
        "\r\n",
        "2. Remove all non-word characters.\r\n",
        "\r\n",
        "3. Remove all punctuations.\r\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ8rys-pubcb"
      },
      "source": [
        "for i in range(len(dataset)): \r\n",
        "    dataset[i] = dataset[i].lower() \r\n",
        "    dataset[i] = re.sub(r'\\W', ' ', dataset[i]) \r\n",
        "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl69Y8-nuuGZ"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "###  **3.Finding most frequent words**\r\n",
        "<hr/>\r\n",
        "\r\n",
        "1. Declare a dictionary to hold our bag of words.\r\n",
        "\r\n",
        "2. Tokenize each sentence to words.\r\n",
        "\r\n",
        "3. Check if the word exists in our dictionary.\r\n",
        "If it does, then increment its count by 1. If it doesnâ€™t, add it to our dictionary and set its count as 1.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwIHGtgyXIJ",
        "outputId": "436faac2-1d08-4f13-92bb-453a8ecbe4ce"
      },
      "source": [
        "\r\n",
        "word2count = {} \r\n",
        "for data in dataset: \r\n",
        "    words = nltk.word_tokenize(data) \r\n",
        "    for word in words: \r\n",
        "        if word not in word2count.keys(): \r\n",
        "            word2count[word] = 1\r\n",
        "        else: \r\n",
        "            word2count[word] += 1\r\n",
        "word2count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 2,\n",
              " 'about': 1,\n",
              " 'and': 3,\n",
              " 'are': 1,\n",
              " 'artificial': 3,\n",
              " 'at': 1,\n",
              " 'based': 1,\n",
              " 'better': 1,\n",
              " 'can': 1,\n",
              " 'common': 1,\n",
              " 'comprehensive': 1,\n",
              " 'democratizing': 1,\n",
              " 'develop': 1,\n",
              " 'developers': 1,\n",
              " 'doing': 1,\n",
              " 'educate': 1,\n",
              " 'empowers': 1,\n",
              " 'for': 2,\n",
              " 'goal': 1,\n",
              " 'gurugram': 1,\n",
              " 'haryana': 1,\n",
              " 'impact': 1,\n",
              " 'in': 2,\n",
              " 'intelligence': 3,\n",
              " 'is': 1,\n",
              " 'its': 1,\n",
              " 'language': 1,\n",
              " 'learning': 1,\n",
              " 'machine': 1,\n",
              " 'make': 1,\n",
              " 'natural': 1,\n",
              " 'of': 1,\n",
              " 'open': 1,\n",
              " 'people': 1,\n",
              " 'platform': 1,\n",
              " 'products': 1,\n",
              " 'research': 1,\n",
              " 'resources': 1,\n",
              " 'robofied': 2,\n",
              " 'safe': 1,\n",
              " 'scope': 1,\n",
              " 'singularity': 1,\n",
              " 'so': 1,\n",
              " 'solutions': 1,\n",
              " 'source': 1,\n",
              " 'speech': 1,\n",
              " 'that': 1,\n",
              " 'the': 1,\n",
              " 'them': 1,\n",
              " 'they': 1,\n",
              " 'towards': 2,\n",
              " 'tutorials': 1,\n",
              " 'via': 1,\n",
              " 'we': 3,\n",
              " 'which': 1,\n",
              " 'working': 1,\n",
              " 'world': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kc0ClJezYUi"
      },
      "source": [
        "<hr>\r\n",
        "\r\n",
        "### **4. Building the Bag of words model**\r\n",
        "<hr/>\r\n",
        "\r\n",
        "Construct a vector, which would tell whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, set it as 1, else set it as 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyug0RrYzy5k",
        "outputId": "bf04b1e6-c291-4ad1-c51d-5ae6a3c53970"
      },
      "source": [
        "import heapq \r\n",
        "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\r\n",
        "X = [] \r\n",
        "for data in dataset: \r\n",
        "    vector = [] \r\n",
        "    for word in freq_words: \r\n",
        "        if word in nltk.word_tokenize(data): \r\n",
        "            vector.append(1) \r\n",
        "        else: \r\n",
        "            vector.append(0) \r\n",
        "    X.append(vector) \r\n",
        "X = np.asarray(X) \r\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4qiyNFF1JUD"
      },
      "source": [
        "### **Bag of Words Model with Sklearn**\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "We can use the CountVectorizer() function from the Sk-learn library to implement the Bag of words model using Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n8g6kJb1m2Z",
        "outputId": "c5f062cd-baf7-42ae-eea8-a4b4149b4597"
      },
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "text = \"Robofied is a comprehensive Artificial Intelligence platform based in Gurugram,Haryana working towards democratizing safe artificial intelligence towards a common goal of Singularity. At Robofied, we are doing research in speech, natural language, and machine learning. We develop open-source solutions for developers which empowers them so that they can make better products for the world. We educate people about Artificial Intelligence, its scope and impact via resources and tutorials.\"\r\n",
        "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\r\n",
        "                           stop_words='english')\r\n",
        "#transform\r\n",
        "Count_data = CountVec.fit_transform([text])\r\n",
        " \r\n",
        "#create dataframe\r\n",
        "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\r\n",
        "print(cv_dataframe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   artificial  based  better  common  ...  speech  tutorials  working  world\n",
            "0           3      1       1       1  ...       1          1        1      1\n",
            "\n",
            "[1 rows x 37 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}